{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPB-openACC-C-BT Implementation\n",
    "In this self-paced, hands-on lab, we will briefly explore some methods for OpenACC\n",
    "\n",
    "Qichao Hong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see get some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be three: 3\n"
     ]
    }
   ],
   "source": [
    "print (\"The answer should be three: \" + str(1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the cell below to get some info about the GPUs on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 23 00:03:06 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 780 Ti  Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 26%   38C    P8    N/A /  N/A |    178MiB /  3017MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 780 Ti  Off  | 0000:02:00.0     N/A |                  N/A |\r\n",
      "| 26%   38C    P8    N/A /  N/A |      1MiB /  3020MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0                  Not Supported                                         |\r\n",
      "|    1                  Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU: Intel i7-4960x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p class=\"hint_trigger\">If you have never before taken an IPython Notebook based self-paced lab from NVIDIA, click this green box.\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The following video will explain the infrastructure we are using for this self-paced lab, as well as give some tips on it's usage.  If you've never taken a lab on this system before, it's highly encourage you watch this short video first.<br><br>\n",
    "<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/ZMrDaLSFqpY\" frameborder=\"0\" allowfullscreen></iframe></div>\n",
    "<br>\n",
    "<h2 style=\"text-align:center;color:red;\">Attention Firefox Users</h2><div style=\"text-align:center; margin: 0px 25px 0px 25px;\">There is a bug with Firefox related to setting focus in any text editors embedded in this lab. Even though the cursor may be blinking in the text editor, focus for the keyboard may not be there, and any keys you press may be applying to the previously selected cell.  To work around this issue, you'll need to first click in the margin of the browser window (where there are no cells) and then in the text editor.  Sorry for this inconvenience, we're working on getting this fixed.</div></div></div></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to OpenACC\n",
    "\n",
    "Open-specification OpenACC directives are a straightforward way to accelerate existing Fortran and C applications. With OpenACC directives, you provide hints via compiler directives (or 'pragmas') to tell the compiler where -- and how -- it should parallelize compute-intensive code for execution on an accelerator. \n",
    "\n",
    "If you've done parallel programming using OpenMP, OpenACC is very similar: using directives, applications can be parallelized *incrementally*, with little or no change to the Fortran, C or C++ source. Debugging and code maintenance are easier. OpenACC directives are designed for *portability* across operating systems, host CPUs, and accelerators. You can use OpenACC directives with GPU accelerated libraries, explicit parallel programming languages (e.g., CUDA), MPI, and OpenMP, *all in the same program.*\n",
    "\n",
    "Watch the following short video introduction to OpenACC:\n",
    "\n",
    "<div align=\"center\"><iframe width=\"640\" height=\"390\" style=\"margin: 0 auto;\" src=\"http://www.youtube.com/embed/c9WYCFEt_Uo\" frameborder=\"0\" allowfullscreen></iframe></div>\n",
    "\n",
    "This hands-on lab walks you through a short sample of a scientific code, and demonstrates how you can employ OpenACC directives using a four-step process. You will make modifications to a simple C program, then compile and execute the newly enhanced code in each step. Along the way, hints and solution are provided, so you can check your work, or take a peek if you get lost.\n",
    "\n",
    "If you are confused now, or at any point in this lab, you can consult the <a href=\"#FAQ\">FAQ</a> located at the bottom of this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Characterize Your Application\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most difficult part of accelerator programming begins before the first line of code is written. If your program is not highly parallel, an accelerator or coprocesor won't be much use. Understanding the code structure is crucial if you are going to *identify opportunities* and *successfully* parallelize a piece of code. The first step in OpenACC programming then is to *characterize the application*. This includes:\n",
    "\n",
    "+ benchmarking the single-thread, CPU-only version of the application\n",
    "+ understanding the program structure and how data is passed through the call tree\n",
    "+ profiling the application and identifying computationally-intense \"hot spots\"\n",
    "    + which loop nests dominate the runtime?\n",
    "    + what are the minimum/average/maximum tripcounts through these loop nests?\n",
    "    + are the loop nests suitable for an accelerator?\n",
    "+ insuring that the algorithms you are considering for acceleration are *safely* parallel\n",
    "\n",
    "Note: what we've just said may sound a little scary, so please note that as parallel programming methods go OpenACC is really pretty friendly: think of it as a sandbox you can play in. Because OpenACC directives are incremental, you can add one or two directives at a time and see how things work: the compiler provides a *lot* of feedback. The right software plus good tools plus educational experiences like this one should put you on the path to successfully accelerating your programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Profiling and Benchmarking\n",
    "\n",
    "Before you start modifying code and adding OpenACC directives, you should benchmark the serial version of the program. To facilitate benchmarking after this and every other step in our parallel porting effort, we have built a timing routine around the main structure of our program -- a process we recommend you follow in your own efforts. Let's run the `BT` file without making any changes -- and see how fast the serial program executes. This will establish a baseline for future comparisons.  Execute the following two cells to compile and run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o *~ mputil* ../common/*.o *.cu *.ptx *.w2c.c *.w2c.h *.x *.i *.t *.spin *.B\n",
      "rm -f  npbparams.h core\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "rm -f setparams setparams.h npbparams.h\n",
      "rm -f *~ *.o\n",
      "cc  -o setparams setparams.c\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "../sys/setparams bt B\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-seq'\n",
      "cc  -c -I../common  -DCRPL_COMP=0 bt.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 initialize.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 exact_solution.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 exact_rhs.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 set_constants.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 adi.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 rhs.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 add.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 error.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 verify.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 print_results.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 c_timers.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 wtime.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 x_solve.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 y_solve.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 z_solve.c\n",
      "cc  -c -I../common  -DCRPL_COMP=0 solve_subs.c\n",
      "cc  -o ./bt.B.x bt.o  initialize.o exact_solution.o exact_rhs.o set_constants.o adi.o  rhs.o add.o error.o verify.o print_results.o c_timers.o wtime.o x_solve.o y_solve.o z_solve.o solve_subs.o   -lm\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-seq'\n"
     ]
    }
   ],
   "source": [
    "!cd ./NPB-acc/BT-seq/ && make clean && make BT CLASS=B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " NAS Parallel Benchmarks (NPB3.3-SER-C) - BT Benchmark\n",
      "\n",
      " No input file inputbt.data. Using compiled defaults\n",
      " Size:  102x 102x 102\n",
      " Iterations:  200    dt:   0.000300\n",
      "\n",
      " Time step    1\n",
      " Time step   20\n",
      " Time step   40\n",
      " Time step   60\n",
      " Time step   80\n",
      " Time step  100\n",
      " Time step  120\n",
      " Time step  140\n",
      " Time step  160\n",
      " Time step  180\n",
      " Time step  200\n",
      " Verification being performed for class B\n",
      " accuracy setting for epsilon =  1.0000000000000E-08\n",
      " Comparison of RMS-norms of residual\n",
      "           1 1.4233597229287E+03 1.4233597229287E+03 7.3482401545462E-15\n",
      "           2 9.9330522590151E+01 9.9330522590150E+01 4.8642556962122E-15\n",
      "           3 3.5646025644535E+02 3.5646025644535E+02 3.0298608016653E-15\n",
      "           4 3.2485447959083E+02 3.2485447959084E+02 2.5547251692915E-14\n",
      "           5 3.2707541254660E+03 3.2707541254659E+03 7.6468922270923E-15\n",
      " Comparison of RMS-norms of solution error\n",
      "           1 5.2969847140937E+01 5.2969847140937E+01 4.0242294858973E-16\n",
      "           2 4.4632896115671E+00 4.4632896115671E+00 1.9899636747710E-16\n",
      "           3 1.3122573342210E+01 1.3122573342210E+01 4.0609950344566E-16\n",
      "           4 1.2006925323559E+01 1.2006925323559E+01 2.9588871281057E-16\n",
      "           5 1.2459576151036E+02 1.2459576151036E+02 2.2811136659766E-16\n",
      " Verification Successful\n",
      "\n",
      "\n",
      " BT Benchmark Completed.\n",
      " Class           =                        B\n",
      " Size            =            102x 102x 102\n",
      " Iterations      =                      200\n",
      " Time in seconds =                   620.85\n",
      " Mop/s total     =                  1131.00\n",
      " Operation type  =           floating point\n",
      " Verification    =               SUCCESSFUL\n",
      " Version         =                    3.3.1\n",
      " Compile date    =              23 May 2017\n",
      "\n",
      " Compile options:\n",
      "    CC           = (none)\n",
      "    CLINK        = (none)\n",
      "    C_LIB        = -lm\n",
      "    C_INC        = -I../common\n",
      "    CFLAGS       = (none)\n",
      "    CLINKFLAGS   = (none)\n",
      "    RAND         = (none)\n",
      "\n",
      "--------------------------------------\n",
      " Please send all errors/feedbacks to:\n",
      " Center for Manycore Programming\n",
      " cmp@aces.snu.ac.kr\n",
      " http://aces.snu.ac.kr\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "======== CPU profiling result (top down):\n",
      "Time(%)      Time  Name\n",
      " 100.00%   626.49s  main\n",
      " 99.60%      624s    adi\n",
      " 28.04%   175.66s    | z_solve\n",
      "  7.06%     44.2s    | | binvcrhs\n",
      "  3.72%    23.31s    | | matmul_sub\n",
      "  0.87%     5.45s    | | matvec_sub\n",
      "  0.07%     410ms    | | lhsinit\n",
      "  0.05%     310ms    | | binvrhs\n",
      " 27.85%   174.49s    | y_solve\n",
      "  7.11%    44.55s    | | binvcrhs\n",
      "  3.70%    23.16s    | | matmul_sub\n",
      "  1.02%     6.36s    | | matvec_sub\n",
      "  0.08%     470ms    | | lhsinit\n",
      "  0.05%     310ms    | | binvrhs\n",
      " 26.73%   167.43s    | x_solve\n",
      "  6.82%     42.7s    | | binvcrhs\n",
      "  3.77%     23.6s    | | matmul_sub\n",
      "  0.85%     5.33s    | | matvec_sub\n",
      "  0.09%     560ms    | | lhsinit\n",
      "  0.05%     290ms    | | binvrhs\n",
      " 16.17%    101.3s    | compute_rhs\n",
      "  0.82%     5.12s    | add\n",
      "  0.19%     1.19s    initialize\n",
      "  0.17%     1.07s    | exact_solution\n",
      "  0.11%     670ms    exact_rhs\n",
      "  0.04%     270ms    | exact_solution\n",
      "  0.10%     630ms    verify\n",
      "  0.08%     500ms      compute_rhs\n",
      "  0.02%     110ms      error_norm\n",
      "  0.01%      90ms      | exact_solution\n",
      "  0.00%      20ms      rhs_norm\n",
      "\n",
      "======== Data collected at 100Hz frequency\n"
     ]
    }
   ],
   "source": [
    "!pgprof --cpu-profiling on --cpu-profiling-mode top-down ./NPB-acc/BT-seq/bt.B.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Checking/Keeping a Record\n",
    "\n",
    "*After each step*, we will record the results from our benchmarking and correctness tests in a table like this one: \n",
    "\n",
    "|Step| Execution       | ExecutionTime (s)     | Speedup vs. 1 CPU Thread       | Correct? | Programming Time |\n",
    "|:--:| --------------- | ---------------------:| ------------------------------:|:--------:| -----------------|\n",
    "|1   | CPU 1 thread    | 620.85           |                                | Yes      |                |  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see x_solve(), y_solve(), z_solve(), and compute_rhs need the most time to compute. So we will work mainly on these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Add Compute Directives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things need to to before you add #pragma ...\n",
    "    1. Initiate the GPU\n",
    "        acc_init(acc_device_default);\n",
    "    2. Create the variables on GPU needed to run\n",
    "        #pragma acc data create(forcing,rho_i,u,us,vs,ws,square,qs,rhs)\n",
    "        {\n",
    "            ...\n",
    "         }\n",
    "    3. In BT, these functions will make changes to array u[], forcing[], . If we don't updates it on GPU, we will get wrong result. \n",
    "        initialize(), exact_rhs().\n",
    "        \n",
    "        Add #pragma acc update device(var) at the end of the function to update relative variable in GPU\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We mainly worked on x_solve, y_solve,  z_solve , and compute_rhs. We add compute directives to these for loops inside these three functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In x_solve , y_solve, z_solve, and compute_rhs:\n",
    "First, to run these functions in GPU, we need to feed the GPU the data it needs. And create some variables that are not in GPU to compute.\n",
    "\n",
    "```\n",
    "#pragma acc data present(rho_i,u,qs,rhs,square) create(lhsX,fjacX,njacX)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "We see all the for loops are doing simple arithmatic action to some size of 5D arrays we can simply add \"#pragma acc parallel loop #\" before the nesty loops to make them parallel (each loop is independent).\n",
    "\n",
    "```\n",
    "#pragma acc parallel loop\n",
    "    for (i = 0; i <= isize; i++) {\n",
    "      for (j = 1; j <= gp12; j++) {\n",
    "        for (k = 1; k <= gp22; k++) {\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o *~ mputil* ../common/*.o *.cu *.ptx *.w2c.c *.w2c.h *.x *.i *.t *.spin *.B\n",
      "rm -f  npbparams.h core\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "rm -f setparams setparams.h npbparams.h\n",
      "rm -f *~ *.o\n",
      "cc  -o setparams setparams.c\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "../sys/setparams bt B\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-step1'\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 bt.c\n",
      "main:\n",
      "    210, Generating create(forcing[:][:][:][:],qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:],us[:][:][:],vs[:][:][:],ws[:][:][:])\n",
      "    239, Generating update self(u[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 initialize.c\n",
      "initialize:\n",
      "    223, Generating update device(u[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 exact_solution.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 exact_rhs.c\n",
      "exact_rhs:\n",
      "    383, Generating update device(forcing[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 set_constants.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 adi.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 rhs.c\n",
      "compute_rhs:\n",
      "     90, Generating present(forcing[:][:][:][:],qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:],us[:][:][:],vs[:][:][:],ws[:][:][:])\n",
      "     93, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "         94, #pragma acc loop gang /* blockIdx.x */\n",
      "         95, #pragma acc loop seq\n",
      "         96, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     95, Loop is parallelizable\n",
      "     96, Loop is parallelizable\n",
      "    116, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        117, #pragma acc loop gang /* blockIdx.x */\n",
      "        118, #pragma acc loop seq\n",
      "        119, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    118, Loop is parallelizable\n",
      "    119, Loop is parallelizable\n",
      "    132, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        133, #pragma acc loop gang /* blockIdx.x */\n",
      "        134, #pragma acc loop seq\n",
      "        135, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    134, Loop is parallelizable\n",
      "    135, Loop is parallelizable\n",
      "    191, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        192, #pragma acc loop gang /* blockIdx.x */\n",
      "        193, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    193, Loop is parallelizable\n",
      "    230, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        231, #pragma acc loop gang /* blockIdx.x */\n",
      "        232, #pragma acc loop seq\n",
      "        233, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    232, Loop is parallelizable\n",
      "    233, Loop is parallelizable\n",
      "    258, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        259, #pragma acc loop gang /* blockIdx.x */\n",
      "        260, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    260, Loop is parallelizable\n",
      "    300, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        301, #pragma acc loop gang /* blockIdx.x */\n",
      "        302, #pragma acc loop seq\n",
      "        303, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    302, Loop is parallelizable\n",
      "    303, Loop is parallelizable\n",
      "    354, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        355, #pragma acc loop gang /* blockIdx.x */\n",
      "        356, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    356, Loop is parallelizable\n",
      "    394, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        395, #pragma acc loop gang /* blockIdx.x */\n",
      "        396, #pragma acc loop seq\n",
      "        397, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    396, Loop is parallelizable\n",
      "    397, Loop is parallelizable\n",
      "    422, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        423, #pragma acc loop gang /* blockIdx.x */\n",
      "        424, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    424, Loop is parallelizable\n",
      "    464, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        465, #pragma acc loop gang /* blockIdx.x */\n",
      "        466, #pragma acc loop seq\n",
      "        467, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    466, Loop is parallelizable\n",
      "    467, Loop is parallelizable\n",
      "    519, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        520, #pragma acc loop gang /* blockIdx.x */\n",
      "        521, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    521, Loop is parallelizable\n",
      "    558, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        559, #pragma acc loop gang /* blockIdx.x */\n",
      "        560, #pragma acc loop seq\n",
      "        561, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    560, Loop is parallelizable\n",
      "    561, Loop is parallelizable\n",
      "    586, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        587, #pragma acc loop gang /* blockIdx.x */\n",
      "        588, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    588, Loop is parallelizable\n",
      "    625, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        626, #pragma acc loop gang /* blockIdx.x */\n",
      "        627, #pragma acc loop seq\n",
      "        628, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    627, Loop is parallelizable\n",
      "    628, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 add.c\n",
      "add:\n",
      "     75, Generating present(rhs[:][:][:][:],u[:][:][:][:])\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "         77, #pragma acc loop gang(gp22) /* blockIdx.x */\n",
      "         79, #pragma acc loop worker(4) /* threadIdx.y */\n",
      "         81, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "     79, Loop is parallelizable\n",
      "     81, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 error.c\n",
      "rhs_norm:\n",
      "    112, Generating update self(rhs[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 verify.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 print_results.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 c_timers.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 wtime.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 x_solve.c\n",
      "x_solve:\n",
      "    102, Generating create(fjacX[:][:][:][:][:],lhsX[:][:][:][:][:][:],njacX[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "    106, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        107, #pragma acc loop gang /* blockIdx.x */\n",
      "        108, #pragma acc loop seq\n",
      "        109, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    108, Loop is parallelizable\n",
      "    109, Loop is parallelizable\n",
      "    194, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        195, #pragma acc loop gang /* blockIdx.x */\n",
      "        196, #pragma acc loop seq\n",
      "        197, #pragma acc loop seq\n",
      "        198, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    196, Loop is parallelizable\n",
      "    197, Loop is parallelizable\n",
      "    198, Loop is parallelizable\n",
      "    212, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        213, #pragma acc loop gang /* blockIdx.x */\n",
      "        214, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    214, Loop is parallelizable\n",
      "    229, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        230, #pragma acc loop gang /* blockIdx.x */\n",
      "        231, #pragma acc loop seq\n",
      "        232, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    231, Loop is parallelizable\n",
      "    232, Loop is parallelizable\n",
      "    421, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        422, #pragma acc loop gang /* blockIdx.x */\n",
      "        423, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    423, Loop is parallelizable\n",
      "    717, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        718, #pragma acc loop gang /* blockIdx.x */\n",
      "        720, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "        721, #pragma acc loop seq\n",
      "    720, Loop is parallelizable\n",
      "    721, Loop carried dependence of lhsX,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsX,rhs prevents vectorization\n",
      "   1199, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1200, #pragma acc loop gang /* blockIdx.x */\n",
      "       1201, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1201, Loop is parallelizable\n",
      "   1245, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1246, #pragma acc loop gang /* blockIdx.x */\n",
      "       1247, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1247, Loop is parallelizable\n",
      "   1392, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1393, #pragma acc loop gang /* blockIdx.x */\n",
      "       1394, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1394, Loop is parallelizable\n",
      "   1556, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1557, #pragma acc loop gang /* blockIdx.x */\n",
      "       1558, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1558, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 y_solve.c\n",
      "y_solve:\n",
      "     96, Generating create(fjacY[:][:][:][:][:],lhsY[:][:][:][:][:][:],njacY[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "    100, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        101, #pragma acc loop gang /* blockIdx.x */\n",
      "        102, #pragma acc loop seq\n",
      "        103, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    102, Loop is parallelizable\n",
      "    103, Loop is parallelizable\n",
      "    186, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        187, #pragma acc loop gang /* blockIdx.x */\n",
      "        188, #pragma acc loop seq\n",
      "        189, #pragma acc loop seq\n",
      "        190, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    188, Loop is parallelizable\n",
      "    189, Loop is parallelizable\n",
      "    190, Loop is parallelizable\n",
      "    204, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        205, #pragma acc loop gang /* blockIdx.x */\n",
      "        206, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    206, Loop is parallelizable\n",
      "    221, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        222, #pragma acc loop gang /* blockIdx.x */\n",
      "        223, #pragma acc loop seq\n",
      "        224, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    223, Loop is parallelizable\n",
      "    224, Loop is parallelizable\n",
      "    409, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        410, #pragma acc loop gang /* blockIdx.x */\n",
      "        411, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    411, Loop is parallelizable\n",
      "    703, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        704, #pragma acc loop gang /* blockIdx.x */\n",
      "        705, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "        706, #pragma acc loop seq\n",
      "    705, Loop is parallelizable\n",
      "    706, Loop carried dependence of lhsY,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsY,rhs prevents vectorization\n",
      "   1186, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1187, #pragma acc loop gang /* blockIdx.x */\n",
      "       1188, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1188, Loop is parallelizable\n",
      "   1232, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1233, #pragma acc loop gang /* blockIdx.x */\n",
      "       1234, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1234, Loop is parallelizable\n",
      "   1379, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1380, #pragma acc loop gang /* blockIdx.x */\n",
      "       1381, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1381, Loop is parallelizable\n",
      "   1541, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1542, #pragma acc loop gang /* blockIdx.x */\n",
      "       1543, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1543, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 z_solve.c\n",
      "z_solve:\n",
      "     96, Generating create(fjacZ[:][:][:][:][:],lhsZ[:][:][:][:][:][:],njacZ[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "     99, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        100, #pragma acc loop gang /* blockIdx.x */\n",
      "        101, #pragma acc loop seq\n",
      "        102, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    101, Loop is parallelizable\n",
      "    102, Loop is parallelizable\n",
      "    185, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        186, #pragma acc loop gang /* blockIdx.x */\n",
      "        187, #pragma acc loop seq\n",
      "        188, #pragma acc loop seq\n",
      "        189, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    187, Loop is parallelizable\n",
      "    188, Loop is parallelizable\n",
      "    189, Loop is parallelizable\n",
      "    203, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        204, #pragma acc loop gang /* blockIdx.x */\n",
      "        205, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    205, Loop is parallelizable\n",
      "    220, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        221, #pragma acc loop gang /* blockIdx.x */\n",
      "        222, #pragma acc loop seq\n",
      "        223, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    222, Loop is parallelizable\n",
      "    223, Loop is parallelizable\n",
      "    411, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        412, #pragma acc loop gang /* blockIdx.x */\n",
      "        413, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "    413, Loop is parallelizable\n",
      "    704, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        705, #pragma acc loop gang /* blockIdx.x */\n",
      "        706, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "        707, #pragma acc loop seq\n",
      "    706, Loop is parallelizable\n",
      "    707, Loop carried dependence of lhsZ,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsZ,rhs prevents vectorization\n",
      "   1194, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1195, #pragma acc loop gang /* blockIdx.x */\n",
      "       1196, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1196, Loop is parallelizable\n",
      "   1240, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1241, #pragma acc loop gang /* blockIdx.x */\n",
      "       1242, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1242, Loop is parallelizable\n",
      "   1386, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1387, #pragma acc loop gang /* blockIdx.x */\n",
      "       1388, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1388, Loop is parallelizable\n",
      "   1553, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1554, #pragma acc loop gang /* blockIdx.x */\n",
      "       1555, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "   1555, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 solve_subs.c\n",
      "pgcc -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -o ./bt.B.x bt.o  initialize.o exact_solution.o exact_rhs.o set_constants.o adi.o  rhs.o add.o error.o verify.o print_results.o c_timers.o wtime.o x_solve.o y_solve.o z_solve.o solve_subs.o   -lm\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-step1'\n"
     ]
    }
   ],
   "source": [
    "!cd ./NPB-acc/BT-step1/ && make clean && make CC=pgcc CLASS=B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get any error please check your work and try re-compilling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiler did a lot for you!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " NAS Parallel Benchmarks (NPB3.3-ACC-C) - BT Benchmark\n",
      "\n",
      " No input file inputbt.data. Using compiled defaults\n",
      " Size:  102x 102x 102\n",
      " Iterations:  200    dt:   0.000300\n",
      "\n",
      " Time step    1\n",
      " Time step   20\n",
      " Time step   40\n",
      " Time step   60\n",
      " Time step   80\n",
      " Time step  100\n",
      " Time step  120\n",
      " Time step  140\n",
      " Time step  160\n",
      " Time step  180\n",
      " Time step  200\n",
      " Verification being performed for class B\n",
      " accuracy setting for epsilon =  1.0000000000000E-08\n",
      " Comparison of RMS-norms of residual\n",
      "           1 1.4233597229287E+03 1.4233597229287E+03 5.2715635891310E-15\n",
      "           2 9.9330522590150E+01 9.9330522590150E+01 2.8613268801248E-16\n",
      "           3 3.5646025644535E+02 3.5646025644535E+02 3.1893271596477E-16\n",
      "           4 3.2485447959083E+02 3.2485447959084E+02 2.3797439933126E-14\n",
      "           5 3.2707541254660E+03 3.2707541254659E+03 5.0052385486422E-15\n",
      " Comparison of RMS-norms of solution error\n",
      "           1 5.2969847140937E+01 5.2969847140937E+01 4.0242294858973E-16\n",
      "           2 4.4632896115671E+00 4.4632896115671E+00 1.9899636747710E-16\n",
      "           3 1.3122573342210E+01 1.3122573342210E+01 4.0609950344566E-16\n",
      "           4 1.2006925323559E+01 1.2006925323559E+01 2.9588871281057E-16\n",
      "           5 1.2459576151036E+02 1.2459576151036E+02 2.2811136659766E-16\n",
      " Verification Successful\n",
      "\n",
      "\n",
      " BT Benchmark Completed.\n",
      " Class           =                        B\n",
      " Size            =            102x 102x 102\n",
      " Iterations      =                      200\n",
      " Time in seconds =                    24.13\n",
      " Mop/s total     =                 29100.06\n",
      " Operation type  =           floating point\n",
      " Verification    =               SUCCESSFUL\n",
      " Version         =                    3.3.1\n",
      " Compile date    =              23 May 2017\n",
      "\n",
      " Compile options:\n",
      "    CC           = (none)\n",
      "    CLINK        = (none)\n",
      "    C_LIB        = -lm\n",
      "    C_INC        = -I../common\n",
      "    CFLAGS       = (none)\n",
      "    CLINKFLAGS   = (none)\n",
      "    RAND         = (none)\n",
      "\n",
      "--------------------------------------\n",
      " Please send all errors/feedbacks to:\n",
      " Center for Manycore Programming\n",
      " cmp@aces.snu.ac.kr\n",
      " http://aces.snu.ac.kr\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ulimit -s unlimited && ./NPB-acc/BT-step1/bt.B.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's record our results in the table:\n",
    "\n",
    "|Step| Execution    | Time(s)     | Speedup vs. 1 CPU Thread  | Correct? | Programming Time |\n",
    "| -- || ------------ | ----------- | ------------------------- | -------- | ---------------- |\n",
    "|1| CPU 1 thread |620.85      |                           |          | |\n",
    "|2| Add kernels directive  |24.13      | 25.73X           | Yes      | ||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Compiler use default setting of gang, worker and vector to run the benchmark.\n",
    "We can still adjust these values manully to let the program fit the device you have.\n",
    "\n",
    "For example:\n",
    "```\n",
    "#pragma acc parallel loop gang num_gangs(isize+1) num_workers(8) vector_length(32)\n",
    "    for (i = 0; i <= isize; i++) {\n",
    "#pragma acc loop worker\n",
    "      for (j = 1; j <= gp12; j++) {\n",
    "#pragma acc loop vector\n",
    "        for (k = 1; k <= gp22; k++) {\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o *~ mputil* ../common/*.o *.cu *.ptx *.w2c.c *.w2c.h *.x *.i *.t *.spin *.B\n",
      "rm -f  npbparams.h core\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "rm -f setparams setparams.h npbparams.h\n",
      "rm -f *~ *.o\n",
      "cc  -o setparams setparams.c\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/sys'\n",
      "../sys/setparams bt B\n",
      "make[1]: Entering directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-final'\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 bt.c\n",
      "main:\n",
      "    210, Generating create(forcing[:][:][:][:],qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:],us[:][:][:],vs[:][:][:],ws[:][:][:])\n",
      "    239, Generating update self(u[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 initialize.c\n",
      "initialize:\n",
      "    223, Generating update device(u[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 exact_solution.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 exact_rhs.c\n",
      "exact_rhs:\n",
      "    383, Generating update device(forcing[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 set_constants.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 adi.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 rhs.c\n",
      "compute_rhs:\n",
      "     90, Generating present(forcing[:][:][:][:],qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:],us[:][:][:],vs[:][:][:],ws[:][:][:])\n",
      "     98, Loop is parallelizable\n",
      "    100, Loop is parallelizable\n",
      "    102, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "         98, #pragma acc loop gang /* blockIdx.y */\n",
      "        100, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        102, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    127, Loop is parallelizable\n",
      "    129, Loop is parallelizable\n",
      "    131, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        127, #pragma acc loop gang /* blockIdx.y */\n",
      "        129, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        131, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    149, Loop is parallelizable\n",
      "    151, Loop is parallelizable\n",
      "    153, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        149, #pragma acc loop gang /* blockIdx.y */\n",
      "        151, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        153, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    214, Loop is parallelizable\n",
      "    216, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        214, #pragma acc loop gang /* blockIdx.x */\n",
      "        216, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    258, Loop is parallelizable\n",
      "    260, Loop is parallelizable\n",
      "    262, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        258, #pragma acc loop gang /* blockIdx.y */\n",
      "        260, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        262, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    292, Loop is parallelizable\n",
      "    294, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        292, #pragma acc loop gang /* blockIdx.x */\n",
      "        294, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    339, Loop is parallelizable\n",
      "    341, Loop is parallelizable\n",
      "    343, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        339, #pragma acc loop gang /* blockIdx.y */\n",
      "        341, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        343, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    399, Loop is parallelizable\n",
      "    401, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        399, #pragma acc loop gang /* blockIdx.x */\n",
      "        401, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    444, Loop is parallelizable\n",
      "    446, Loop is parallelizable\n",
      "    448, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        444, #pragma acc loop gang /* blockIdx.y */\n",
      "        446, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        448, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    478, Loop is parallelizable\n",
      "    480, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        478, #pragma acc loop gang /* blockIdx.x */\n",
      "        480, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    525, Loop is parallelizable\n",
      "    527, Loop is parallelizable\n",
      "    529, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        525, #pragma acc loop gang /* blockIdx.y */\n",
      "        527, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        529, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    586, Loop is parallelizable\n",
      "    588, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        586, #pragma acc loop gang /* blockIdx.x */\n",
      "        588, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    630, Loop is parallelizable\n",
      "    632, Loop is parallelizable\n",
      "    634, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        630, #pragma acc loop gang /* blockIdx.y */\n",
      "        632, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        634, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "    664, Loop is parallelizable\n",
      "    666, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        664, #pragma acc loop gang /* blockIdx.x */\n",
      "        666, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.y threadIdx.y threadIdx.x */\n",
      "    708, Loop is parallelizable\n",
      "    710, Loop is parallelizable\n",
      "    712, Loop is parallelizable\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        708, #pragma acc loop gang /* blockIdx.y */\n",
      "        710, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */\n",
      "        712, #pragma acc loop gang, vector(32) /* blockIdx.z threadIdx.x */\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 add.c\n",
      "add:\n",
      "     75, Generating present(rhs[:][:][:][:],u[:][:][:][:])\n",
      "         Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "         77, #pragma acc loop gang(gp22) /* blockIdx.x */\n",
      "         79, #pragma acc loop worker(4) /* threadIdx.y */\n",
      "         81, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "     79, Loop is parallelizable\n",
      "     81, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 error.c\n",
      "rhs_norm:\n",
      "    112, Generating update self(rhs[:][:][:][:])\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 verify.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 print_results.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 c_timers.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 wtime.c\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 x_solve.c\n",
      "x_solve:\n",
      "    102, Generating create(fjacX[:][:][:][:][:],lhsX[:][:][:][:][:][:],njacX[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "    106, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        108, #pragma acc loop gang(isize+1) /* blockIdx.x */\n",
      "        110, #pragma acc loop worker(8) /* threadIdx.y */\n",
      "        112, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    110, Loop is parallelizable\n",
      "    112, Loop is parallelizable\n",
      "    197, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        199, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "        201, #pragma acc loop vector(32), worker(8) /* threadIdx.x threadIdx.\n",
      "        202, #pragma acc loop seq\n",
      "        203, #pragma acc loop seq\n",
      "    201, Loop is parallelizable\n",
      "    202, Loop is parallelizable\n",
      "    203, Loop is parallelizable\n",
      "    217, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        219, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "        221, #pragma acc loop vector(32), worker(8) /* threadIdx.x threadIdx.\n",
      "    221, Loop is parallelizable\n",
      "    236, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        238, #pragma acc loop gang(isize-1) /* blockIdx.x */\n",
      "        240, #pragma acc loop worker(8) /* threadIdx.y */\n",
      "        242, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    240, Loop is parallelizable\n",
      "    242, Loop is parallelizable\n",
      "    431, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        433, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "        435, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "    435, Loop is parallelizable\n",
      "    729, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        731, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "        734, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "        735, #pragma acc loop seq\n",
      "    734, Loop is parallelizable\n",
      "    735, Loop carried dependence of lhsX,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsX,rhs prevents vectorization\n",
      "   1213, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1215, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1217, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1217, Loop is parallelizable\n",
      "   1261, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1263, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1265, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1265, Loop is parallelizable\n",
      "   1410, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1412, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1414, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1414, Loop is parallelizable\n",
      "   1576, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1578, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1580, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1580, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 y_solve.c\n",
      "y_solve:\n",
      "     96, Generating create(fjacY[:][:][:][:][:],lhsY[:][:][:][:][:][:],njacY[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rho_i[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "    100, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        102, #pragma acc loop gang(jsize+1) /* blockIdx.x */\n",
      "        104, #pragma acc loop worker(8) /* threadIdx.y */\n",
      "        106, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    104, Loop is parallelizable\n",
      "    106, Loop is parallelizable\n",
      "    189, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        191, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        193, #pragma acc loop vector(32), worker(8) /* threadIdx.x threadIdx.\n",
      "        194, #pragma acc loop seq\n",
      "        195, #pragma acc loop seq\n",
      "    193, Loop is parallelizable\n",
      "    194, Loop is parallelizable\n",
      "    195, Loop is parallelizable\n",
      "    209, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        211, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        213, #pragma acc loop vector(32), worker(8) /* threadIdx.x threadIdx.\n",
      "    213, Loop is parallelizable\n",
      "    228, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        230, #pragma acc loop gang(jsize-1) /* blockIdx.x */\n",
      "        232, #pragma acc loop worker(4) /* threadIdx.y */\n",
      "        234, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    232, Loop is parallelizable\n",
      "    234, Loop is parallelizable\n",
      "    419, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        421, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        423, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "    423, Loop is parallelizable\n",
      "    715, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        717, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        719, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "        720, #pragma acc loop seq\n",
      "    719, Loop is parallelizable\n",
      "    720, Loop carried dependence of lhsY,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsY,rhs prevents vectorization\n",
      "   1200, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1202, #pragma acc loop gang(gp22) /* blockIdx.x */\n",
      "       1204, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1204, Loop is parallelizable\n",
      "   1248, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1250, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "       1252, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1252, Loop is parallelizable\n",
      "   1397, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1399, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "       1401, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1401, Loop is parallelizable\n",
      "   1561, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1563, #pragma acc loop gang(gp22) /* blockIdx.x */\n",
      "       1565, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1565, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 z_solve.c\n",
      "z_solve:\n",
      "     96, Generating create(fjacZ[:][:][:][:][:],lhsZ[:][:][:][:][:][:],njacZ[:][:][:][:][:])\n",
      "         Generating present(qs[:][:][:],rhs[:][:][:][:],square[:][:][:],u[:][:][:][:])\n",
      "     99, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        101, #pragma acc loop gang(ksize+1) /* blockIdx.x */\n",
      "        103, #pragma acc loop worker(4) /* threadIdx.y */\n",
      "        105, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    103, Loop is parallelizable\n",
      "    105, Loop is parallelizable\n",
      "    188, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        190, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        192, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "        193, #pragma acc loop seq\n",
      "        194, #pragma acc loop seq\n",
      "    192, Loop is parallelizable\n",
      "    193, Loop is parallelizable\n",
      "    194, Loop is parallelizable\n",
      "    208, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        210, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        212, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "    212, Loop is parallelizable\n",
      "    227, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        229, #pragma acc loop gang(ksize-1) /* blockIdx.x */\n",
      "        231, #pragma acc loop worker(4) /* threadIdx.y */\n",
      "        233, #pragma acc loop vector(32) /* threadIdx.x */\n",
      "    231, Loop is parallelizable\n",
      "    233, Loop is parallelizable\n",
      "    421, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        423, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        425, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "    425, Loop is parallelizable\n",
      "    716, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "        718, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "        720, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "        721, #pragma acc loop seq\n",
      "    720, Loop is parallelizable\n",
      "    721, Loop carried dependence of lhsZ,rhs prevents parallelization\n",
      "         Loop carried backward dependence of lhsZ,rhs prevents vectorization\n",
      "   1208, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1210, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "       1212, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1212, Loop is parallelizable\n",
      "   1256, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1258, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1260, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1260, Loop is parallelizable\n",
      "   1404, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1406, #pragma acc loop gang(gp02) /* blockIdx.x */\n",
      "       1408, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1408, Loop is parallelizable\n",
      "   1573, Accelerator kernel generated\n",
      "         Generating Tesla code\n",
      "       1575, #pragma acc loop gang(gp12) /* blockIdx.x */\n",
      "       1577, #pragma acc loop vector(32), worker(4) /* threadIdx.x threadIdx.\n",
      "   1577, Loop is parallelizable\n",
      "pgcc  -c -I../common -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -DCRPL_COMP=0 solve_subs.c\n",
      "pgcc -O3 -acc -ta=nvidia,cc35,cuda8.0  -Minfo=accel -mcmodel=medium -o ./bt.B.x bt.o  initialize.o exact_solution.o exact_rhs.o set_constants.o adi.o  rhs.o add.o error.o verify.o print_results.o c_timers.o wtime.o x_solve.o y_solve.o z_solve.o solve_subs.o   -lm\n",
      "make[1]: Leaving directory '/home/qichao/Desktop/notebooks-acc/NPB-acc/BT-final'\n"
     ]
    }
   ],
   "source": [
    "!cd ./NPB-acc/BT-final/ && make clean && make CC=pgcc CLASS=B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " NAS Parallel Benchmarks (NPB3.3-ACC-C) - BT Benchmark\n",
      "\n",
      " No input file inputbt.data. Using compiled defaults\n",
      " Size:  102x 102x 102\n",
      " Iterations:  200    dt:   0.000300\n",
      "\n",
      " Time step    1\n",
      " Time step   20\n",
      " Time step   40\n",
      " Time step   60\n",
      " Time step   80\n",
      " Time step  100\n",
      " Time step  120\n",
      " Time step  140\n",
      " Time step  160\n",
      " Time step  180\n",
      " Time step  200\n",
      " Verification being performed for class B\n",
      " accuracy setting for epsilon =  1.0000000000000E-08\n",
      " Comparison of RMS-norms of residual\n",
      "           1 1.4233597229287E+03 1.4233597229287E+03 5.2715635891310E-15\n",
      "           2 9.9330522590150E+01 9.9330522590150E+01 2.8613268801248E-16\n",
      "           3 3.5646025644535E+02 3.5646025644535E+02 3.1893271596477E-16\n",
      "           4 3.2485447959083E+02 3.2485447959084E+02 2.3797439933126E-14\n",
      "           5 3.2707541254660E+03 3.2707541254659E+03 5.0052385486422E-15\n",
      " Comparison of RMS-norms of solution error\n",
      "           1 5.2969847140937E+01 5.2969847140937E+01 4.0242294858973E-16\n",
      "           2 4.4632896115671E+00 4.4632896115671E+00 1.9899636747710E-16\n",
      "           3 1.3122573342210E+01 1.3122573342210E+01 4.0609950344566E-16\n",
      "           4 1.2006925323559E+01 1.2006925323559E+01 2.9588871281057E-16\n",
      "           5 1.2459576151036E+02 1.2459576151036E+02 2.2811136659766E-16\n",
      " Verification Successful\n",
      "\n",
      "\n",
      " BT Benchmark Completed.\n",
      " Class           =                        B\n",
      " Size            =            102x 102x 102\n",
      " Iterations      =                      200\n",
      " Time in seconds =                    22.97\n",
      " Mop/s total     =                 30571.59\n",
      " Operation type  =           floating point\n",
      " Verification    =               SUCCESSFUL\n",
      " Version         =                    3.3.1\n",
      " Compile date    =              23 May 2017\n",
      "\n",
      " Compile options:\n",
      "    CC           = (none)\n",
      "    CLINK        = (none)\n",
      "    C_LIB        = -lm\n",
      "    C_INC        = -I../common\n",
      "    CFLAGS       = (none)\n",
      "    CLINKFLAGS   = (none)\n",
      "    RAND         = (none)\n",
      "\n",
      "--------------------------------------\n",
      " Please send all errors/feedbacks to:\n",
      " Center for Manycore Programming\n",
      " cmp@aces.snu.ac.kr\n",
      " http://aces.snu.ac.kr\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ulimit -s unlimited && ./NPB-acc/BT-final/bt.B.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's record our results in the table:\n",
    "\n",
    "|Step| Execution    | Time(s)     | Speedup vs. 1 CPU Thread  | Correct? | Programming Time |\n",
    "| -- || ------------ | ----------- | ------------------------- | -------- | ---------------- |\n",
    "|1| CPU 1 thread |620.85      |                           |          | |\n",
    "|2| Add parallel directive  |24.13      | 25.73X           | Yes      | |\n",
    "|3| Optimization  |22.97      | 27.03X           | Yes      | ||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for BT, the compiler did very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
